{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL de la página principal\n",
    "BASE_URL = \"http://books.toscrape.com/\"\n",
    "\n",
    "def scrape_books(url):\n",
    "    \"\"\"\n",
    "    Función para scrapear los libros de una página.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: No se pudo acceder a {url}\")\n",
    "        return []\n",
    "\n",
    "    # Parsear el contenido HTML\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Lista para guardar los datos\n",
    "    books = []\n",
    "\n",
    "    # Encontrar todos los libros en la página\n",
    "    for book in soup.find_all('article', class_='product_pod'):\n",
    "        # Extraer el título del libro\n",
    "        title = book.h3.a['title']\n",
    "        \n",
    "        # Extraer el precio\n",
    "        price = book.find('p', class_='price_color').text.strip()\n",
    "        \n",
    "        # Extraer disponibilidad\n",
    "        availability = book.find('p', class_='instock availability').text.strip()\n",
    "        \n",
    "        # Guardar los datos del libro\n",
    "        books.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'availability': availability\n",
    "        })\n",
    "    \n",
    "    return books\n",
    "\n",
    "def scrape_all_pages():\n",
    "    \"\"\"\n",
    "    Función para scrapear todos los libros en todas las páginas.\n",
    "    \"\"\"\n",
    "    page = 1\n",
    "    all_books = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scrapeando página {page}...\")\n",
    "        url = f\"{BASE_URL}catalogue/page-{page}.html\"\n",
    "        books = scrape_books(url)\n",
    "        \n",
    "        if not books:\n",
    "            break  # Salir del bucle si no hay libros (fin de las páginas)\n",
    "        \n",
    "        all_books.extend(books)\n",
    "        page += 1\n",
    "\n",
    "    return all_books\n",
    "\n",
    "def save_to_csv(books, filename='books.csv'):\n",
    "    \"\"\"\n",
    "    Guardar los datos en un archivo CSV.\n",
    "    \"\"\"\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['title', 'price', 'availability'])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(books)\n",
    "\n",
    "    print(f\"Datos guardados en {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Scrape todos los libros\n",
    "    all_books = scrape_all_pages()\n",
    "\n",
    "    # Guardar los datos en un archivo CSV\n",
    "    save_to_csv(all_books)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
